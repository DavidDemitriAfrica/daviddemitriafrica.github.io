---
layout: default
title: "Research - David Demitri Africa"
---

<h1>Research</h1>

<p>Below is all my currently published research work. Each entry includes a link to the paper, a short summary, and some personal thoughts.</p>

<div class="research-entry">
    <h2><a href="https://doi.org/10.1063/5.0166785" target="_blank">Lag and Duration of Leaderâ€“Follower Relationships in Mixed Traffic Using Causal Inference</a></h2>
    <p><strong>Summary:</strong> This paper implements a causal inference approach to analyze leader-follower dynamics in an arterial road in Chennai, India. We quantify the temporal lag and duration of interactions using transfer entropy metrics.</p>
    <p><strong>My thoughts:</strong> This was my first paper. I learned a lot about how to write research in general from here, and in the future I think I would like to do more papers in this style-- analyzing some weird real world phenomenon with an interesting method. Published in Chaos.</p>
</div>

<div class="research-entry">
    <h2><a href="https://aclanthology.org/2025.acl-long.1509/" target="_blank">Batayan: A Filipino NLP benchmark for evaluating Large Language Models
</a></h2>
    <p><strong>Summary:</strong> This paper introduces Batayan, a benchmark to evaluate LLMs on NLP tasks in Filipino.</p>
    <p><strong>My thoughts:</strong> Most of the work here was in actually writing/re-translating the entries. Would be nice to do some in-depth error analysis ala 
Parser Showdown at the Wall Street Corral. Published in ACL 2025 Main Conference</p>
</div>

<div class="research-entry">
    <h2><a href="https://arxiv.org/abs/2506.22105" target="_blank">Identifying a Circuit for Verb Conjugation in GPT-2
</a></h2>
    <p><strong>Summary:</strong> Looking for a circuit in GPT-2 that does subject verb agreement. We find one, but it gets progressively larger as the SVA task gets more complicated.</p>
    <p><strong>My thoughts:</strong> Final project for L193: Explainable Artificial Intelligence. Thinking of a place to submit this.</p>
</div>

<div class="research-entry">
    <h2><a href="https://arxiv.org/abs/2506.23679" target="_blank">Learning Modular Exponentiation with Transformers
</a></h2>
    <p><strong>Summary:</strong> We teach a small 4-layer transformer modular exponentiation. PCA on embeddings doesn't show any clear structure, but we do find a cool example of grokking by multiples of moduli. Also, we find a small circuit that performs regular/normal exponentiation.</p>
    <p><strong>My thoughts:</strong> Final project for R252: Theory of Deep Learning. Thinking of a place to submit this.</p>
</div>

<style>
.research-entry {
    margin-bottom: 2em;
    padding-bottom: 1em;
    border-bottom: 1px solid #ccc;
}
</style>
